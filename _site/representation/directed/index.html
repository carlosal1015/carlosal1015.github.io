<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Bayesian networks</title>
  <meta name="description" content="Lecture notes for Stanford cs228.">


  <link rel="stylesheet" href="/css/tufte.css">	
  

  <!-- Google Fonts loaded here depending on setting in _data/options.yml true loads font, blank does not-->
  
    <link href='//fonts.googleapis.com/css?family=Lato:400,400italic' rel='stylesheet' type='text/css'>
  
  <!-- Load up MathJax script if needed ... specify in /_data/options.yml file-->
  
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  

  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-75587219-1', 'auto');
  ga('send', 'pageview');

  </script>

  <link rel="canonical" href="https://carlosal1015.github.io//representation/directed/">
  <link rel="alternate" type="application/rss+xml" title="Curso de elementos finitos" href="https://carlosal1015.github.io//feed.xml" />
</head>

  <body>
    <!--- Header and nav template site-wide -->
<header>
    <nav class="group">
        <a href="/">Contents</a>
	<a href="http://cs.stanford.edu/~ermon/cs228/index.html">Class</a>
	<a href="http://github.com/kuleshov/cs228-notes">Github</a>
	</nav>
</header>

    <article class="group">
      <h1>Bayesian networks</h1>
<p class="subtitle"></p>


<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      Macros: {
        e: "\\epsilon",
        xti: "x^{(i)}",
        yti: "y^{(i)}",
        bfy: "{\\bf y}",
        bfx: "{\\bf x}",
        bfg: "{\\bf g}",
        bfbeta: "{\\bf \\beta}",
        tp: "\\tilde p",
        pt: "p_\\theta",
        Exp: "{\\mathbb{E}}",
        Ind: "{\\mathbb{I}}",
        KL: "{\\mathbb{KL}}",
        Dc: "{\\mathcal{D}}",
        Tc: "{\\mathcal{T}}",
        Xc: "{\\mathcal{X}}",
        note: ["\\textcolor{blue}{[NOTE: #1]}",1]
      }
    }
  });
</script>


<p>We begin our study of probabilistic graphical models with the topic of <em>representation</em>.
The main question we want to address is the following: how do we define a probability distribution that models some real-world phenomenon of interest? This is not a trivial problem: we have seen that a naive model for classifying spam messages with <script type="math/tex">n</script> possible words involves potentially up to <script type="math/tex">O(2^n)</script> parameters.</p>

<p>In this chapter, we will:</p>

<ul>
  <li>Propose a general approach for parametrizing probability distributions using only a few parameters.</li>
  <li>Introduce a visual language for describing and representing these distributions via <em>directed acyclic graphs</em> (DAGs).</li>
  <li>Study how our graphical representation describes properties of the probability distribution; this will enable to us to accurately understand our modeling assumptions and will later help designing efficient inference algorithms.</li>
</ul>

<p>We will later see that there are two general approaches to representing probabilities.
The first approach, which we will see here, describes probabilities using directed graphs, also referred to as <em>Bayesian networks</em>. The second approach, covered in the next chapter, will involve undirected graphs, which are also called <em>Markov random fields</em>.</p>

<h2 id="probabilistic-modeling-with-bayesian-networks">Probabilistic modeling with Bayesian networks</h2>

<p>Directed graphical models (also known as Bayesian networks) are a class of probability distributions that admit a particular compact parametrization that can be elegantly described using directed graphs. The general idea behind this parametrization is that each variable depends only on a small number of <em>ancestor</em> variables.</p>

<p>Recall that by the chain rule, we can write any probability <script type="math/tex">p</script> as:</p>
<div class="mathblock"><script type="math/tex; mode=display">
p(x_1,x_2,...,x_n) = p(x_1) p(x_2\mid x_1) \cdots p(x_n\mid x_{n-1},...,x_2,x_1).
</script></div>
<p>A compact Bayesian network is a distribution in which each factor on the right hand side depends only on a small number of <em>ancestor variables</em> <script type="math/tex">x_{A_i}</script>:</p>
<div class="mathblock"><script type="math/tex; mode=display"> 
p(x_i \mid  x_{i-1}...x_1) = p(x_i \mid  x_{A_i}). 
</script></div>
<p>For example, in a model with five variables, we may choose to approximate the factor <span>​<script type="math/tex">p(x_5\mid x_4, x_3, x_2, x_1)</script></span> with <span>​<script type="math/tex">p(x_5 \mid  x_4, x_3)</script></span>. In this case, we write <script type="math/tex">x_{A_i} = \{x_4, x_3\}</script>.</p>

<p>When the variables are discrete (which will be often be the case in the problem we will consider), we may think of the factors <span>​<script type="math/tex">p(x_i\mid x_{A_i})</script></span> as <em>probability tables</em>, in which columns correspond to assignments to <script type="math/tex">x_{A_i}</script> and rows correspond to values of <script type="math/tex">x_i</script>; the entries contain the actual probabilities <span>​<script type="math/tex">p(x_i\mid x_{A_i})</script></span>. If each variable takes <script type="math/tex">d</script> values and has at most <script type="math/tex">k</script> ancestors, then the entire table will contain at most <script type="math/tex">O(d^{k+1})</script> entries. Since we have one table per variable, the entire probability distribution can be compactly described with only <script type="math/tex">O(nd^k)</script> parameters (compared to <script type="math/tex">O(d^n)</script> with a naive approach).</p>

<h3 id="graphical-representation">Graphical representation.</h3>

<p>Distributions of this form can be naturally expressed as <em>directed acyclic graphs</em>, in which vertices correspond to variables <script type="math/tex">x_i</script> and edges indicate dependency relationships. In particular we set the parents of each node to <script type="math/tex">x_i</script> to its ancestors <script type="math/tex">x_{A_i}</script>.</p>

<p>As an example, consider a model of a student’s grade <script type="math/tex">g</script> on an exam; in addition to <script type="math/tex">g</script>, we also model other aspects of the problem, such as the exam’s difficulty <script type="math/tex">d</script>, the student’s intelligence <script type="math/tex">i</script>, his SAT score <script type="math/tex">s</script>, and the quality <script type="math/tex">l</script> of a reference letter from the professor who taught the course. Each variable is binary, except for <script type="math/tex">g</script>, which takes 3 possible values.<label for="nb1" class="margin-toggle">⊕</label><input type="checkbox" id="nb1" class="margin-toggle" /><span class="marginnote"><img class="fullwidth" src="/assets/img/grade-model.png" /><br />Bayes net model describing the performance of a student on an exam. The distribution can be represented a product of conditional probability distributions specified by tables. The form of these distributions is described by edges in the graph.</span> The joint probability distribution over the 5 variables naturally factorizes as follows:</p>
<div class="mathblock"><script type="math/tex; mode=display">
p(l, g, i, d, s) = p(l \mid  g) p(g \mid  i, d) p(i) p(d) p(s\mid d).
</script></div>
<p>The graphical representation of this distribution is a DAG that visually specifies how random variables depend on each other. The graph clearly indicates that the letter depends on the grade, which in turn depends on the student’s intelligence and the difficulty of the exam.</p>

<p>Another way to interpret directed graphs is in terms of stories for how the data was generated.
In the above example, to determine the quality of the reference letter, we may first sample an intelligence level and an exam difficulty; then, a student’s grade is sampled given these parameters; finally, the recommendation letter is generated based on that grade.
In the previous spam classification example, we implicitly postulated that email is generated according to a two-step process:
first, we choose a spam/non-spam label <script type="math/tex">y</script>; then we sample independently whether each word is present, conditioned on that label.</p>

<h3 id="formal-definition">Formal definition.</h3>

<p>Formally, a Bayesian network is a directed graph <script type="math/tex">G = (V,E)</script> together with</p>

<ul>
  <li>A random variable <script type="math/tex">x_i</script> for each node <script type="math/tex">i \in V</script>.</li>
  <li>One conditional probability distribution (CPD) <span>​<script type="math/tex">p(x_i \mid  x_{A_i})</script></span> per node, specifying the probability of <script type="math/tex">x_i</script> conditioned on its parents’ values.</li>
</ul>

<p>Thus, a Bayesian network defines a probability distribution <script type="math/tex">p</script>.
Conversely, we say that a probability <script type="math/tex">p</script> <em>factorizes</em> over a DAG <script type="math/tex">G</script> if it can be decomposed into a product of factors, as specified by <script type="math/tex">G</script>.</p>

<p>It is not hard to see that a probability represented by a Bayesian network will be valid: clearly, it will be non-negative and one can show using an induction argument (and using the fact that the CPDs are valid probabilities) that the sum over all variable assignments will be one.
Conversely, we can also show by counter-example that when <script type="math/tex">G</script> contains cycles, its associated probability may not sum to one.</p>

<h2 id="the-dependencies-of-a-bayes-net">The dependencies of a Bayes net</h2>

<p>To summarize, Bayesian networks represent probability distributions that can be formed via products of smaller, local conditional probability distributions (one for each variable). 
By expressing a probability in this form, we are introducing into our model assumptions that certain variables are independent.</p>

<p>This raises the question: which independence assumptions are we exactly making by using a model Bayesian network with a given structure described by <script type="math/tex">G</script>?
This question important for two reasons: we should know precisely what model assumptions we are making (and whether they are correct); also, this information will help us design more efficient inference algorithms later on.</p>

<p>Let us use the notation <script type="math/tex">I(p)</script> to denote the set of all conditional independencies that hold for a joint distribution <script type="math/tex">p</script>. For example, if <script type="math/tex">p(x,y)=p(x)p(y)</script>, then we say that <script type="math/tex">x \perp y \in I(p)</script>.</p>

<h3 id="independencies-described-by-directed-graphs">Independencies described by directed graphs</h3>

<p>It turns our that a Bayesian network <script type="math/tex">p</script> very elegantly describes many independencies in <script type="math/tex">I(p)</script>; these independencies can be recovered from the graph by looking at three types of structures.</p>

<p>For simplicity, let’s start by looking at a Bayes net <script type="math/tex">G</script> with three nodes: <script type="math/tex">A</script>, <script type="math/tex">B</script>, and <script type="math/tex">C</script>. In this case, <script type="math/tex">G</script> essentially has only three possible structures, each of which leads to different independence assumptions. The interested reader can easily prove these results using a bit of algebra.</p>

<ul>
  <li><label for="bn" class="margin-toggle">⊕</label><input type="checkbox" id="bn" class="margin-toggle" /><span class="marginnote"><img class="fullwidth" src="/assets/img/3node-bayesnets.png" /><br />Bayesian networks over three variables, encoding different types of dependencies: cascade (a,b), common parent (c), and v-structure (d).</span><em>Common parent.</em> If <script type="math/tex">G</script> is of the form <script type="math/tex">A \leftarrow B \rightarrow C</script>, and <script type="math/tex">B</script> is observed, then <span>​<script type="math/tex">A \perp C \mid  B</script></span>. However, if <script type="math/tex">B</script> is unobserved, then <script type="math/tex">A \not\perp C</script>. Intuitively this stems from the fact that <script type="math/tex">B</script> contains all the information that determines the outcomes of <script type="math/tex">A</script> and <script type="math/tex">C</script>; once it is observed, there is nothing else that affects these variables’ outcomes.</li>
  <li><em>Cascade</em>: If <script type="math/tex">G</script> equals <script type="math/tex">A \rightarrow B \rightarrow C</script>, and <script type="math/tex">B</script> is again observed, then, again <span>​<script type="math/tex">A \perp C \mid  B</script></span>. However, if <script type="math/tex">B</script> is unobserved, then <script type="math/tex">A \not\perp C</script>. Here, the intuition is again that <script type="math/tex">B</script> holds all the information that determines the outcome of <script type="math/tex">C</script>; thus, it does not matter what value <script type="math/tex">A</script> takes.</li>
  <li><em>V-structure</em> (also known as <em>explaining away</em>): If <script type="math/tex">G</script> is <script type="math/tex">A \rightarrow C \leftarrow B</script>, then knowing <script type="math/tex">C</script> couples <script type="math/tex">A</script> and <script type="math/tex">B</script>. In other words, <script type="math/tex">A \perp B</script> if <script type="math/tex">C</script> is unobserved, but <span>​<script type="math/tex">A \not\perp B \mid  C</script></span> if <script type="math/tex">C</script> is observed.</li>
</ul>

<p>The latter case requires additional explanation. Suppose that <script type="math/tex">C</script> is a Boolean variable that indicates whether our lawn is wet one morning; <script type="math/tex">A</script> and <script type="math/tex">B</script> are two explanation for it being wet: either it rained (indicated by <script type="math/tex">A</script>), or the sprinkler turned on (indicated by <script type="math/tex">B</script>). If we know that the grass is wet (<script type="math/tex">C</script> is true) and the sprinkler didn’t go on (<script type="math/tex">B</script> is false), then the probability that <script type="math/tex">A</script> is true must be one, because that is the only other possible explanation. Hence, <script type="math/tex">C</script> and <script type="math/tex">A</script> are not independent given <script type="math/tex">B</script>.</p>

<p>These structures clearly describe the independencies encoded by a three-variable Bayesian net. We can extend them to general networks by applying them recursively over any larger graph. This leads to a notion called <script type="math/tex">d</script>-separation (where <script type="math/tex">d</script> stands for directed).</p>

<p>We say that <script type="math/tex">Q</script>, <script type="math/tex">W</script> are <script type="math/tex">d</script>-separated when variables <script type="math/tex">O</script> are observed if they are connected by an <em>active path</em>. An undirected path in the Bayesian Network structure <script type="math/tex">G</script> is called <em>active</em> given observed variables <script type="math/tex">O</script> if for every consecutive triple of variables <script type="math/tex">X,Y,Z</script> on the path, one of the following holds:</p>

<ul>
  <li><script type="math/tex">X \leftarrow Y \leftarrow Z</script>, and <script type="math/tex">Y</script> is unobserved <script type="math/tex">Y \not\in O</script></li>
  <li><script type="math/tex">X \rightarrow Y \rightarrow Z</script>, and <script type="math/tex">Y</script> is unobserved <script type="math/tex">Y \not\in O</script></li>
  <li><script type="math/tex">X \leftarrow Y \rightarrow Z</script>, and <script type="math/tex">Y</script> is unobserved <script type="math/tex">Y \not\in O</script></li>
  <li><script type="math/tex">X \leftarrow Y \leftarrow Z</script>, and <script type="math/tex">Y</script> or any of its descendents are observed.
<label for="dp2" class="margin-toggle">⊕</label><input type="checkbox" id="dp2" class="margin-toggle" /><span class="marginnote"><img class="fullwidth" src="/assets/img/dsep2.png" /><br />In this example, <script type="math/tex">X_1</script> and <script type="math/tex">X_6</script> are <script type="math/tex">d</script>-separated given <script type="math/tex">X_2, X_3</script>.</span><label for="dp1" class="margin-toggle">⊕</label><input type="checkbox" id="dp1" class="margin-toggle" /><span class="marginnote"><img class="fullwidth" src="/assets/img/dsep1.png" /><br />However, <script type="math/tex">X_2, X_3</script> are not <script type="math/tex">d</script>-separated given <script type="math/tex">X_1, X_6</script>. There is an active pass which passed through the V-structure created when <script type="math/tex">X_6</script> is observed.</span></li>
</ul>

<p>For example, in the graph below, <script type="math/tex">X_1</script> and <script type="math/tex">X_6</script> are <script type="math/tex">d</script>-separated given <script type="math/tex">X_2, X_3</script>. However, <script type="math/tex">X_2, X_3</script> are not <script type="math/tex">d</script>-separated given <script type="math/tex">X_2, X_3</script>, because we can find an active path <script type="math/tex">(X_2, X_6, X_5, X_3)</script></p>

<p>The notion of <script type="math/tex">d</script>-separation is  useful, because it lets us describe a large fraction of the dependencies that hold in our model.
Let <span>​<script type="math/tex">I(G) = \{(X \perp Y \mid  Z) : \text{$$X,Y$$ are $$d$$-sep given $$Z$$}\}</script></span> be a set of variables that are <script type="math/tex">d</script>-separated in <script type="math/tex">G</script>.</p>

<p><strong>Fact</strong><label for="1" class="margin-toggle sidenote-number"></label><input type="checkbox" id="1" class="margin-toggle" /><span class="sidenote">We will not formally prove this, but the intuition is that if <script type="math/tex">X,Y</script> and <script type="math/tex">Y,Z</script> are mutually dependent, so are <script type="math/tex">X,Z</script>. Thus we can look at adjacent nodes and propagate dependencies according to the local dependency structures outlined above. </span>:
If <script type="math/tex">p</script> factorizes over <script type="math/tex">G</script>, then <script type="math/tex">I(G) \subseteq I(p)</script>. In this case, we say that <script type="math/tex">G</script> is an <script type="math/tex">I</script>-map (independence map) for <script type="math/tex">p</script></p>

<p>In other words, all the independencies encoded in <script type="math/tex">G</script> are sound: variables that are <script type="math/tex">d</script>-separated in <script type="math/tex">G</script> are truly independent in <script type="math/tex">p</script>. However, the converse is not true: a distribution may factorize over <script type="math/tex">G</script>, yet have independencies that are not captured in <script type="math/tex">G</script>.</p>

<p>In a way this is almost a trivial statement. If <script type="math/tex">p(x,y) = p(x)p(y)</script>, then this distribution still factorizes over the graph <script type="math/tex">y \rightarrow x</script>, since we can always write it as <span>​<script type="math/tex">p(x,y) = p(x\mid y)p(y)</script></span> with a CPD <span>​<script type="math/tex">p(x\mid y)</script></span> in which the probability of <script type="math/tex">x</script> does not actually vary with <script type="math/tex">y</script>. However, we can construct a graph that matches the structure of <script type="math/tex">p</script> by simply removing that unnecessary edge.</p>



    </article>
    <span class="print-footer">Bayesian networks - Oromion</span>
    <footer>
  <hr class="slender">
  <!-- <ul class="footer&#45;links"> -->
  <!--   <li><a href="mailto:hate@spam.net"><span class="icon&#45;mail"></span></a></li>     -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//www.twitter.com/twitter_handle"><span class="icon-twitter"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//plus.google.com/+googlePlusName"><span class="icon-googleplus"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//github.com/GithubHandle"><span class="icon-github"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//www.flickr.com/photos/FlickrUserID"><span class="icon-flickr"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="/feed"><span class="icon-feed"></span></a> -->
  <!--     </li> -->
  <!--      -->
  <!-- </ul> -->
<div class="credits">
<!-- <span>&#38;copy; 2018 <!&#45;&#45; &#38;#38;nbsp;&#38;#38;nbsp;OROMION &#45;&#45;></span></br> <br> -->
<span>Site created with <a href="//jekyllrb.com">Jekyll</a> using the <a href="//github.com/clayh53/tufte-jekyll">Tufte theme</a>. &copy; 2018</span> 
</div>  
</footer>

  </body>
</html>
